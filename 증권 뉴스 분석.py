import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import re
from datetime import date
from krwordrank.word import KRWordRank
from wordcloud import WordCloud

# ìš”ì•½
from typing import List
from konlpy.tag import Okt
from textrankr import TextRank

# ê°ì„± ë¶„ì„
from argparse import Namespace
from sklearn.metrics import classification_report
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSequenceClassification

import base64

# ë ˆì´ì•„ì›ƒ ì„¤ì •í•˜ê¸°
st.set_page_config(layout='wide')

# í˜ê¾¸!!
st.markdown('<style>body { background-color: #ece6cc; }</style>', unsafe_allow_html=True)

# ê¸ˆìš”ì¼ì¡° ì´ë¦„ ë„£ê¸°!
image_path = 'title.PNG'
sub_path = 'subtitle.PNG'
home_button_image = f'<a href="." style="text-decoration: none;"><img src="data:image/jpeg;base64,{base64.b64encode(open(image_path, "rb").read()).decode()}" alt="Image" width="450" height="110" style="margin-right: 10px;"></a>'
st.markdown(f'<h1 style="text-align: center; margin-top: -100px;">{home_button_image}</h1>', unsafe_allow_html=True)
st.markdown(f'<h2 style="text-align: center; margin-top: -20px;"><img src="data:image/jpeg;base64,{base64.b64encode(open(sub_path, "rb").read()).decode()}" alt="Image" width="600" height="55" style="margin-right: 10px;"></h2>', unsafe_allow_html=True)
# pip install krwordrank 
# pip install wordcloud 

#pip install textrankr 
#pip install konlpy
#pip install transformers


# <í‚¤ì›Œë“œ ì¶”ì¶œ>
# Mecab ì „ì²˜ë¦¬ íŒŒì¼ ì‚¬ìš© ì´í›„ 
# ë¬¸ì¥ë¶€í˜¸ ë° ë¶ˆìš©ì–´ ì œê±° ì „ì²˜ë¦¬

# <ìš”ì•½ ë° ê°ì„± ë¶„ì„>
# ì›ë¬¸ ë°ì´í„° ì „ì²˜ë¦¬ 


# <í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™” ê³¼ì •>
# í¬ë¡¤ë§í•œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('final_clean_mecab.csv',index_col=0)

# ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•  ë‚ ì§œ ì„ íƒ
min_date = date(2023, 4, 11)  # ì„ íƒ ê°€ëŠ¥í•œ ê°€ì¥ ì´ë¥¸ ë‚ ì§œ
max_date = date(2023, 7, 11)  # ì„ íƒ ê°€ëŠ¥í•œ ê°€ì¥ ëŠ¦ì€ ë‚ ì§œ
default_date = date(2023, 7, 11)
input_date = st.date_input("ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í™•ì¸í•  ë‚ ì§œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.", min_value=min_date, max_value= max_date, value= default_date)
# ë¶„ì¥ ë¶€í˜¸ ì œê±° í•¨ìˆ˜
def remove_non_alphanumeric(text):
    # í•œê¸€ê³¼ ì˜ì–´ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ì œê±°í•˜ëŠ” ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
    pattern = r'[^ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z\s]'

    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ í•œê¸€ê³¼ ì˜ì–´ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ì œê±°í•˜ê³  ë°˜í™˜
    cleaned_text = re.sub(pattern, '', text)
    return cleaned_text

# DataFrameì˜ 'cleaned_mecab' ì—´ì— ìˆëŠ” ëª¨ë“  ë¬¸ìì—´ì— ëŒ€í•´ í•œê¸€ê³¼ ì˜ì–´ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ì œê±°
df['cleaned_mecab'] = df['cleaned_mecab'].apply(remove_non_alphanumeric)

# ê¸€ììˆ˜ê°€ 1ê°œì¸ ë‹¨ì–´ ì œê±° í•¨ìˆ˜
def remove_single_character_words(text):
    # ê¸€ììˆ˜ê°€ í•œ ê°œì¸ ë‹¨ì–´ë¥¼ ì œê±°í•˜ëŠ” ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
    pattern = r'\b\w\b'

    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê¸€ììˆ˜ê°€ í•œ ê°œì¸ ë‹¨ì–´ë¥¼ ì œê±°í•˜ê³  ë°˜í™˜
    cleaned_text = re.sub(pattern, '', text)
    return cleaned_text
# ë„ì–´ì“°ê¸°ê°€ 1ê°œ ì´ìƒì¸ ë¶€ë¶„ì„ ë‹¨ì¼ ë„ì–´ì“°ê¸°ë¡œ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜

def remove_extra_spaces(text):
    # ë„ì–´ì“°ê¸°ê°€ 1ê°œ ì´ìƒì¸ ë¶€ë¶„ì„ ë‹¨ì¼ ë„ì–´ì“°ê¸°ë¡œ ë³€ê²½í•˜ëŠ” ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
    pattern = r'\s+'

    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë„ì–´ì“°ê¸°ê°€ 1ê°œ ì´ìƒì¸ ë¶€ë¶„ì„ ë‹¨ì¼ ë„ì–´ì“°ê¸°ë¡œ ë³€ê²½í•˜ê³  ë°˜í™˜
    cleaned_text = re.sub(pattern, ' ', text)
    return cleaned_text

def generate_wordcloud(num):
    # í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥ ì„¤ì •
    text = df.loc[num, 'cleaned_mecab']

    # Substring graphë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ìµœì†Œ ë“±ì¥ ë¹ˆë„ìˆ˜ì™€ ìµœëŒ€ ê¸¸ì´ ì…ë ¥
    min_count = 3   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜ (ê·¸ë˜í”„ ìƒì„± ì‹œ)
    max_length = 10 # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´
    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)

    beta = 0.85    # PageRankì˜ decaying factor beta
    max_iter = 10  # ë°˜ë³µíšŸìˆ˜ ì œí•œ
    texts = text
    keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter)

   
    # ì™¼ìª½ ì—´ì— í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ë¥¼ ì¶œë ¥
    col1, col2 = st.columns(2)
    keyword_df = pd.DataFrame(columns=['í‚¤ì›Œë“œ', 'ë¹„ìœ¨ (ìƒìœ„ 30ê°œê¹Œì§€)'])
    i = 0

    # í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
    if keywords:
        with col1:
            st.write("í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼:")
            for word, r in sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:30]:
                r_rounded = round(r, 2)  # ì†Œìˆ˜ì  ì…‹ ì§¸ ìë¦¬ì—ì„œ ë°˜ì˜¬ë¦¼
                keyword_df.loc[i] = [word, r_rounded]
                i += 1
            st.write(keyword_df)
        
        # ë¶ˆìš©ì–´ ì²˜ë¦¬
        with open('koreanStopwords.txt', 'r', encoding='utf-8') as f:
            stopwords = [line.strip() for line in f]

        passwords = {word: score for word, score in sorted(
            keywords.items(), key=lambda x: -x[1])[:300] if not (word in stopwords)}

        # í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ê°€ 0ê°œê°€ ì•„ë‹ ë•Œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
        if passwords:
            # í•œê¸€ í°íŠ¸ ì‚¬ìš©
            from matplotlib import font_manager, rc
            font_path = "C:\Windows\Fonts/gulim.ttc" 
            font_name = font_manager.FontProperties(fname=font_path).get_name()
            rc('font', family=font_name)

            # ì˜¤ë¥¸ìª½ ì—´ì— ì›Œë“œí´ë¼ìš°ë“œ ê²°ê³¼ë¥¼ ì¶œë ¥
            with col2:
                st.write('')
                krwordrank_cloud = WordCloud(
                    font_path=font_path,
                    width=400,
                    height=400,
                    background_color="white"
                )

                krwordrank_cloud.generate_from_frequencies(passwords)
                plt.figure(figsize=(5, 5))
                plt.imshow(krwordrank_cloud, interpolation="bilinear")
                plt.axis("off")
                st.pyplot(plt)
        else:
            with col2:
                st.write("í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        with col1:
            st.write("í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


#<í…ìŠ¤íŠ¸ ìš”ì•½ ê³¼ì •>
def remove_brackets(text):
    pattern = r'\[.*?\]|\(.*?\)|\<.*?\>'
    result = re.sub(pattern, '', text)
    return result

def remove_special_characters(text):
    pattern = r'[^\w\s.]'
    cleaned_text = re.sub(pattern, '', text)
    return cleaned_text

def process_sentences(text):
    pattern = r'\.'
    grouped_sentences = re.split(pattern, text)
    sentences = [sentence.strip() + '.' for sentence in grouped_sentences if sentence.strip()]
    return sentences

def remove_last_sentence(sentences):
    if sentences:
        del sentences[-1]
        del sentences[-1]
    return sentences


### ìµœì¢… ì „ì²˜ë¦¬ ì½”ë“œ
def processed():
    df = pd.read_csv('final_ss.csv', encoding='utf-8')

    ### 1. [],(),<> ê¸°í˜¸ì™€ ì•ˆì— ìˆëŠ” í•„ìš”ì—†ëŠ” ë¬¸ì ì œê±°í•˜ê¸°
    df['main_text'] = df['main_text'].apply(remove_brackets)

    ### 2. ì•ŒíŒŒë²³, ìˆ«ì, ë°‘ì¤„, ê³µë°±, ê·¸ë¦¬ê³  '.' ë¬¸ìë¥¼ ì œì™¸í•œ ëª¨ë“  ë‹¤ë¥¸ ë¬¸ì ëª¨ë‘ ì‚­ì œ
    df['main_text'] = df['main_text'].apply(remove_special_characters)

    ### 3. ë¬¸ì¥ ì´ì¤‘ë¦¬ìŠ¤íŠ¸ë¡œ ë‹´ê³  '.' ê¸°í˜¸ ë¶™ì´ê¸°
    df['processed_text'] = df['main_text'].apply(process_sentences)

    ### 4. ëª¨ë“  í–‰ì— ëŒ€í•´ [0][-1]ì— ìœ„ì¹˜í•œ ë¬¸ì¥ ì‚­ì œ
    df['processed_text'] = df['processed_text'].apply(remove_last_sentence)

    ### 5.main_textì˜ ' 'ê¸°í˜¸ ì‚­ì œí•˜ê¸°
    df['main_text'] = df['main_text'].apply(lambda x: x.replace("'", ""))

    ### 6. str í˜•íƒœë¡œ ë³€í™˜
    df['processed_text'] = df['processed_text'].str.join(' ')

    cleaned_data = df['processed_text']
    return cleaned_data



# ìš”ì•½ ì½”ë“œ
def textrank_summary(num, cleaned_data):

  class OktTokenizer:
      def __init__(self):
          self.okt = Okt()

      def __call__(self, text: str) -> List[str]:
          tokens: List[str] = self.okt.phrases(text)
          return tokens


  text = cleaned_data[num]

  mytokenizer = OktTokenizer()  # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  textrank = TextRank(mytokenizer)

  k = 3  # num sentences in the resulting summary

  summaries = textrank.summarize(text, k, verbose=False)  # ìš”ì•½ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´

  for summary in summaries:
      if summary.startswith(','):
          summary = summary[1:]  # ì²« ë²ˆì§¸ ë¬¸ìì¸ ','ë¥¼ ì œì™¸í•œ ë¬¸ìì—´ë¡œ ì¬í• ë‹¹
      st.write(summary+".")

# ê°ì„± ë¶„ë¥˜ ëª¨ë¸
def textrank_summary_bert(num, cleaned_data, model, tokenizer):
    class OktTokenizer:
        def __init__(self):
            self.okt = Okt()

        def __call__(self, text: str) -> List[str]:
            tokens: List[str] = self.okt.phrases(text)
            return tokens

    text = cleaned_data[num]

    mytokenizer = OktTokenizer()  # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    textrank = TextRank(mytokenizer)

    k = 5  # num sentences in the resulting summary

    summaries = textrank.summarize(text, k, verbose=False)  # ìš”ì•½ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´

    for i, summary in enumerate(summaries):
        if summary.startswith(','):
            summary = summary[1:]  # ì²« ë²ˆì§¸ ë¬¸ìì¸ ','ë¥¼ ì œì™¸í•œ ë¬¸ìì—´ë¡œ ì¬í• ë‹¹
        summaries[i] = summary
    return summaries


def sentiment_analysis_summary(summaries, model, tokenizer):
    classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
    combined_summary = " ".join(summaries)
    result = classifier(combined_summary)

    result[0]["score"] = "{:.3f}".format(result[0]["score"]) 
    if result[0]['label'] == 'positive': 
        result[0]['label'] = 'ê¸ì •'
        st.write(f'##### â‘¡ ê°ì„± ë¶„ì„ ê²°ê³¼ : {result[0]["label"]} [ì •í™•ë„ : {result[0]["score"]}]')
        
    elif result[0]['label'] == 'negative': 
        result[0]['label'] = 'ë¶€ì •'
        st.write(f'##### â‘¡ ê°ì„± ë¶„ì„ ê²°ê³¼ : {result[0]["label"]} [ì •í™•ë„ : {result[0]["score"]}]')
    else : 
        result[0]['label'] = 'ì¤‘ë¦½'
        st.write(f'##### â‘¡ ê°ì„± ë¶„ì„ ê²°ê³¼ : {result[0]["label"]} [ì •í™•ë„ : {result[0]["score"]}]')
        
########################################

if st.button('ê²€ìƒ‰'):
    st.markdown("------")
    # ì„ íƒ ë‚ ì§œë¡œ ë°ì´í„° í•„í„°ë§
    # ë°ì´í„°í”„ë ˆì„ì˜ 'date' ì»¬ëŸ¼ ê°’ê³¼ ì…ë ¥í•œ ë‚ ì§œë¥¼ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë¹„êµ
    selected_date_str = input_date.strftime('%Y.%m.%d')

    # ë°ì´í„°í”„ë ˆì„ì˜ 'date' ì»¬ëŸ¼ ê°’ í˜•ì‹ í†µì¼
    df['date'] = df['date'].str.replace(' ', '')
    df['date'] = pd.to_datetime(df['date'], format='%Y.%m.%d')

    # ë°ì´í„°í”„ë ˆì„ì—ì„œ ì„ íƒí•œ ë‚ ì§œì™€ ì¼ì¹˜í•˜ëŠ” ë°ì´í„° ì¶”ì¶œ
    filtered_data = df[df['date'] == selected_date_str]
    filtered_data1 = filtered_data.copy()
    filtered_data1 = filtered_data1.rename(columns={'title': 'ì œëª©','main_text': 'ë‚´ìš©'})
    st.write(f'### {input_date}  ì‚¼ì„±ì „ì ì¦ê¶Œ ë‰´ìŠ¤ ğŸ“°')
    st.write('')
    st.write(filtered_data1[['ì œëª©','ë‚´ìš©']])
    st.markdown("------")

    # DataFrameì˜ 'cleaned_mecab' ì—´ì— ìˆëŠ” ëª¨ë“  ë¬¸ìì—´ì— ëŒ€í•´ ê¸€ììˆ˜ê°€ í•œ ê°œì¸ ë‹¨ì–´ë¥¼ ì œê±°
    df['cleaned_mecab'] = df['cleaned_mecab'].apply(remove_single_character_words)

    # DataFrameì˜ 'cleaned_mecab' ì—´ì— ìˆëŠ” ëª¨ë“  ë¬¸ìì—´ì— ëŒ€í•´ ë„ì–´ì“°ê¸°ê°€ 1ê°œ ì´ìƒì¸ ë¶€ë¶„ì„ ë‹¨ì¼ ë„ì–´ì“°ê¸°ë¡œ ë³€ê²½
    df['cleaned_mecab'] = df['cleaned_mecab'].apply(remove_extra_spaces)

    # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©
    df['cleaned_mecab'] = df['cleaned_mecab'].apply(remove_non_alphanumeric)
    df['cleaned_mecab'] = df['cleaned_mecab'].apply(remove_single_character_words)
    df['cleaned_mecab'] = df['cleaned_mecab'].apply(remove_extra_spaces)
    # ëª¨ë“  í–‰ list of str í˜•íƒœë¡œ ë‹´ê¸°
    df['cleaned_mecab'] = df['cleaned_mecab'].apply(lambda x: [x])

    # í‚¤ì›Œë“œ ì¶”ì¶œ ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜

    # ëª¨ë¸ ê°ì²´ ìƒì„±
    tokenizer = AutoTokenizer.from_pretrained("snunlp/KR-FinBert-SC")
    model = AutoModelForSequenceClassification.from_pretrained("snunlp/KR-FinBert-SC")

    # ì„ íƒí•œ ë‚ ì§œì˜ ë‰´ìŠ¤ì— ê´€í•œ ê¸°ëŠ¥ ìµœì¢… ì‹¤í–‰

    st.write(f"### {input_date}  ì‚¼ì„±ì „ì ì¦ê¶Œ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ ğŸ§")
    st.write('')
    for index, row in filtered_data.iterrows():
        # ì„ íƒí•œ ë‚ ìì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ì¶œë ¥
        formatted_date = row['date'].strftime('%Y-%m-%d')  # ë‚ ì§œë¥¼ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        st.write(f"##### â‘  ì œëª© :  {row['title']}")
        

        # ê°ì„± ë¶„ë¥˜
        st.write('')
        cleaned_data=processed()
        summaries = textrank_summary_bert(index, cleaned_data, model, tokenizer)
        sentiment_analysis_summary(summaries, model, tokenizer)
        st.write('')

        # ìš”ì•½
        st.write(f"##### â‘¢ ìš”ì•½")
        data = processed()
        textrank_summary(index, data)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì›Œë“œ í´ë¼ìš°ìŠ¤ ì‹œê°í™”
        st.write('')
        st.write(f"##### â‘£ í‚¤ì›Œë“œ í™•ì¸")
        generate_wordcloud(index)

        

        # ì›ë¬¸ í•˜ì´í¼ë§í¬
        st.write('')
        st.write(f"##### â‘¤ ì›ë¬¸ : [{row['link']}]({row['link']})")
        st.write("-" * 50)





st.markdown('<h3 style="text-align: center;">â—ï¸ ë‰´ìŠ¤ ì •ë³´ ìš”ì•½ ì„œë¹„ìŠ¤ ê°œìš” ì„¤ëª… â—ï¸</h3>', unsafe_allow_html=True)
st.write('')
st.write('')
st.write('##### 1ï¸âƒ£ 3ì¤„ ìš”ì•½: ì›í•˜ëŠ” ì¢…ëª©ì˜ ë‰´ìŠ¤ ì œëª© í´ë¦­ ì‹œ í•´ë‹¹ ê¸°ì‚¬ì˜ ë³¸ë¬¸ì„ 3ì¤„ë¡œ ì••ì¶•í•œ ìš”ì•½ë¬¸ì„ ì œê³µí•©ë‹ˆë‹¤.')
st.write('')
st.write('##### 2ï¸âƒ£ í•µì‹¬ í‚¤ì›Œë“œ: ì›í•˜ëŠ” ì¢…ëª©ì˜ ë‰´ìŠ¤ ì œëª© í´ë¦­ ì‹œ ê¸°ì‚¬ ìš”ì•½ê³¼ í•¨ê»˜ ê¸°ì‚¬ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.')
st.write('')
st.write('##### 3ï¸âƒ£ ë‰´ìŠ¤ ê¸°ì‚¬ ê°ì„± ë¶„ì„: ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•´ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ì„¸ê°€ì§€ì˜ ê°ì •ìœ¼ë¡œ ë‚˜ëˆˆ ë’¤, í•´ë‹¹ ë‰´ìŠ¤ê°€ ì–´ë–¤ ê°ì •ì˜ ë°˜ì‘ì„ ë³´ì´ëŠ” ë‰´ìŠ¤ì¸ì§€ ì•Œ ìˆ˜ ìˆëŠ” ê°ì„±ì ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.')
st.write('-'*100)
